---
title: |
  ![](figures/barcelona.png){width=9in} 
subtitle: "Day 1: Introduction"
author: "Przemyslaw Biecek"
format:
  revealjs: 
    theme: [default]
    slide-number: true
    touch: true
    scrollable: true
    chalkboard: 
      buttons: false
    logo: figures/XAI.png
    footer: BarcelonaTech Summer School -- 19/06/2023
---

# When? Who? Why? What? How? 

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width="70%", fig.width = 8, fig.height = 5.5)
```

```{css, echo=FALSE}
.reveal {
  font-size: 24px;
  line-height: 1.6!important;
}
code {
  font-size: 18px!important;
  line-height: 1.2!important;
}
pre {
  line-height: 1.2!important;
}
```

<br><br><br>

![](mid/mid_how_01.png)

## When? - Agenda

Feel free to interrupt me and ask questions during the workshop!

**Day 1:**

- *Lecture:* Introduction to eXplainable Machine Learning, why it is important.  
- *Lab:* Development of ML models that will be later used for model analysis

**Day 2:**

- *Lecture:* Introduction to local methods LIME and Break-Down / SHAP
- *Lab:* Analysis of selected models with methods for local attribution

**Day 3:**

- *Lecture:* Introduction to Ceteris Paribus / Partial Dependence / ALE methods
- *Lab:* Analysis of selected models with methods for profile analysis

**Day 4:** 

- *Lecture:* Introduction to Variable Importance and Fairness
- *Lab:* Analysis of selected models with methods for variable importance

**Day 5:**

- Presentation of projects and discussion


## Who? - Let's get to know each other! 1/2

:::: {.columns}

::: {.column width="67%"}
**Przemys≈Çaw Biecek**


Support humans' effectiveness through safe, ethical, effective and automated predictions 
by 
developing processes, methods, tools, and software for responsible machine learning.


- works at *Faculty of Mathematics, Informatics, and Mechanics* at University of Warsaw and *Faculty of Mathematics and Information Science* at Warsaw University of Technology, Poland
- for 20 years worked with teams of physicians helping them analyze data and build predictive models (often very simple)
- research interests include **Responsible Machine Learning** and eXplainable Artificial Intelligence 
- (also) worked in R&D teams at large and small corporations such as Samsung, IBM, Netezza, Disney, iQuor
- leads the work of the MI2.AI research team, which carries out XAI related research projects (**looking for collaborators**)
- in free time, he writes stories and comics in the Beta and Bit series [https://www.mi2.ai/beta-bit.html](https://www.mi2.ai/beta-bit.html)
:::

::: {.column width="33%"}
![](figures/przemek8.png)
:::

::::


## Who? - Let's get to know each other! 2/2

![](figures/menti.png)



## Why should I care? 1/7

### Models, models, more models ...

- For a long time in the media, data, machine learning and artificial intelligence were uncritically glorified
- The dominant narrative was that almost every problem can be solved having enough data
- Serious people were making statements like "there is no point in training radiologists, because they will be replaced by AI"
- As with other bubbles, anything that is (star)AI(star) raised (unhealthy) attention 
- The media raced to announce what new problem AI had been solved

<p><img src="images/XAI_01.png" width="100%"></p>


## Why should I care? 2/7

### ... however, not every model works ...

![ ](figures/XAI_12.png)

Every major company developing AI solutions experienced some failures. The slide is from a StatNews report on the implementation of IBM Watson for Oncology. Despite enormous resources and even greater hopes, the system was not well received by doctors. Read more: [https://www.statnews.com/](https://www.statnews.com/wp-content/uploads/2018/09/IBMs-Watson-recommended-unsafe-and-incorrect-cancer-treatments-STAT.pdf)


## Why should I care? 3/7

There is tremendous potential in AI, **but**:

- there is a growing list of examples in which, despite initial bursts of promise, AI systems did not perform as expected
- good results on training data did not transfer to real-world data
- at this point we could discuss various examples of spectacular failures of AI for the next two hours (see ,,Weapons of Math Destruction'' by Cathy O'Neil)

![](figures/incidentdatabase.png){fig-align="center"}

Source: [https://incidentdatabase.ai/](https://incidentdatabase.ai/)


## Why should I care? 4/7

### This should not happen

- How do we know what the model has learned? Maybe it bases decisions on some strange artifact?
- This is not a made up possibility, in the example below the model's decisions correlated strongly with the fact that there were captions in the lower left corner.
- It turns out that in the learning data there was often a description in the lower left corner next to the horse pictures. Instead of learning to recognize the characteristics of horses, it is much easier to recognize the presence of text in the lower left corner.

Read more: [Unmasking Clever Hans predictors and assessing what machines really learn](https://www.nature.com/articles/s41467-019-08987-4)

<p><center><img src="images/CleverHans.png" width="100%"></center></p>


## Why should I care? 5/7

### Sometimes users differ on how the system should work

- Users may have different expectations about how the system should work
- For example, a system showing a job ads for truck drivers, presenting these ads more often to men aged 20-50. Is this an example of age and gender discrimination or an increase in the chance of getting an employee?
- Who should define who should watch the selected advertisement and when

Read more: [https://www.propublica.org](https://www.propublica.org/article/facebook-ads-can-still-discriminate-against-women-and-older-workers-despite-a-civil-rights-settlement)

<p><img src="images/fairness_02.png" width="100%"></p>

## Why should I care? 6/7

### Number of published articles on XAI

- The number of papers related to XAI is growing rapidly
- It is not only new methods but also processes, practices, examples of application in various disciplines

Read more: [A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks. Applied Sciences 2022](https://www.mdpi.com/2076-3417/12/3/1353)

<p><center><img src="images/articles.png" width="80%"></center></p>

## Why should I care? 7/7

- XAI for users

  1. basic right
  2. increases trust

- XAI for developers

  3. debug models
  4. extract knowledge


## XAI for users: regulate AI

- For several years, the European Commission has been working on a so-called AI Act to regulate the use of automated algorithms within the European Union.
- The act includes specific expectations related to the explainability of decisions supported by automated decision-making systems 

Read more: [https://eur-lex.europa.eu](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)

<p><img src="images/XAI_07.png" width="100%"></p>

---

### The right to an explanation in Europe

From [Recital 71 EU GDPR](https://www.privacy-regulation.eu/en/recital-71-GDPR.htm)

,,(71) The data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.

Such processing includes 'profiling' that consists of any form of automated processing of personal data evaluating the personal aspects relating to a natural person, in particular to analyse or predict aspects concerning the data subject's performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements, where it produces legal effects concerning him or her or similarly significantly affects him or her.

However, decision-making based on such processing, including profiling, should be allowed where expressly authorised by Union or Member State law to which the controller is subject, including for fraud and tax-evasion monitoring and prevention purposes conducted in accordance with the regulations, standards and recommendations of Union institutions or national oversight bodies and to ensure the security and reliability of a service provided by the controller, or necessary for the entering or performance of a contract between the data subject and a controller, or when the data subject has given his or her explicit consent.

**In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision**.''



## XAI for developers

![ ](figures/XAI_13.png)


## What? 

**IrResponsible Machine Learning:** 

1. Select a problem. 
2. Optimize the predictive performance of a model on the test set. 
3. Don't test anything else. 
4. Jump to another problem. 

**Responsible Machine Learning:** 

1. Select a problem. 
2. Optimize the predictive performance of a model on the test set. 
3. Verify that the model has not accidentally learned artifacts present in the data. 
4. Verify that it is consistent with domain knowledge. 
5. Monitor the model, because the future is usually different from the training data.

The purpose of this tutorial is to **present techniques for model exploration, visualisation and explanation**. To do this we will use some interesting real-world data, train a few models on the data and then use **XAI** (eXplainable artificial intelligence) techniques to explore these models. 


---

### There is no one-size-fits-all solution

- We will talk about how to identify the needs of different stakeholders and match them with explanatory techniques
- It's still area that needs more active research, there's a lot of talk about user needs, but the available methods are more aimed at model developers

Read more: [Transparency, Auditability and eXplainability of Machine Learning Models in Credit Scoring](https://www.tandfonline.com/doi/abs/10.1080/01605682.2021.1922098)

<p><center><img src="images/Transparency.png" width="100%"></center></p>


---

### How to think about the explainability of predictive models


When we think about the interpretability of models we usually distinguish three classes of methods

- **Interpretable by design,** i.e. methods whose structure allows us to directly analyze how the prediction was formed. For different classes of models, explanations may look different, but they are directly based on model parameters. For linear models they are coefficients, for k-neighbors they are neighbors, for naive Bayes they are marginal distributions
- **model specific,** i.e. methods whose structure is complex but can be summarized or represented to better understand the relationship between input and output. The two most common classes of models with model-specific explanations are tree model committees (here we can summarize the tree structure) and neural networks (here we can usually summarize the flow of the signal through the network)
- **model agnostic,** i.e. methods to which this course is devoted, methods that assume nothing about the structure of the model and can be used for models with different structures. Moreover, they can be used to compare models with different structures.


Read more: [arxiv.org/2009.13248](https://arxiv.org/pdf/2009.13248.pdf)

---

![ ](figures/map_0_2_1.png)

---

![ ](figures/DALEXpiramide 2.png)


## How to get there ?

### DARPA program for development of XAI methods

- You may know DARPA for developing computer mouse (1964), GPS (1983), Internet -- ARPANet (1969) or drones (1988).
- In 2017, DARPA launched a major program to fund projects focused on Explainable Artificial Intelligence (XAI) in particular on AI-human collaboration. Research funded by this program is still ongoing and the program itself has contributed to the growing interest in XAI topics.
- It is worth reading about the assumptions and concepts of this program, many of the ideas are still (after 5 years) valid and attractive research topics.

Read more: [https://www.darpa.mil/](https://www.darpa.mil/program/explainable-artificial-intelligence)

<p><center><img src="images/XAI_10.png" width="100%"></center></p>

---

### Responsible and ethical AI - the business response

- Interestingly, this line of research was very quickly get the interest of business.
- On the websites of many companies dealing with AI-related products and services, you can find bookmarks with the topic of "Trustworthy AI". 
- On the slide we have a few sites of companies producing software for (Auto)ML, namely H2O, we have consulting companies such as McKinsey, PWC, IBM, as well as product companies such as Samsung and Tensorflow. 
- Many companies are outdoing themselves in presenting their principals which include slogans such as Transparency, Fairness, Explaianbility. How can these slogans be realized? 

<p><center><img src="images/XAI_11.png" width="100%"></center></p>

---

### Initiatives to increase the effectiveness of AI applications are being undertaken by various organizations

- Not only business, but also large international organizations are actively working to promote AI solutions that are safe and transparent 

<p><center><img src="images/XAI_09.png" width="100%"></center></p>



## How? - Design Principles

- The workshop consists of 1/3 lecture, 1/3 code examples discussed by the tutor and 1/3 computer-based exercises for participants. 

- It aims to present a set of methods for the exploration of complex predictive models.
I assume that participants are familiar with R or Python and have some basic knowledge of predictive models. In this workshop, we will show how to explore these models.

- Feel free to interrupt me and ask questions during the workshop!

- To make working with models more enjoyable, the materials are based on a true story, which we will tell with the help of a comic strips.


---

![ ](figures/rml_comic_01.png)


# Hands-on

<br><br><br>

![](mid/mid_hands_01.png)


## Your Project

In this course, you will have the opportunity to test XAI tools on your own predictive project.
In the hands-on parts, I will show examples of models for mortality due to covid and you will try to replicate these analyses on a different classification or regression problem.

On Friday (the last day of our class) you will be able to show the results for your models and we can discuss them together.
For the exercises, you can build the models on any dataset from the list below, or choose your own dataset.

- **Barcelona HousingPrices**. Build a model that predicts price per square meter [https://www.kaggle.com/datasets/jorgeglez/barcelona-idealista-housingprices](https://www.kaggle.com/datasets/jorgeglez/barcelona-idealista-housingprices)

- **User Car Prices Barcelona 2022**. Predict price of a car
[https://www.kaggle.com/datasets/erenakbulut/user-car-prices-barcelona-2022](https://www.kaggle.com/datasets/erenakbulut/user-car-prices-barcelona-2022)

- **Barcelona Accidents**.  [https://www.kaggle.com/datasets/marcvelmer/barcelona-accidents](https://www.kaggle.com/datasets/marcvelmer/barcelona-accidents)

- **Titanic**. Use the `titanic` data from the `DALEX` package. This is also a binary classification task (recommended for beginners).

- **FIFA**.  Use the `fifa` data from the `DALEX` package. This is a regression task (recommended for recommended for intermediate).

- **Find your own predictive problem...**



## Model Development Process 

The model development is an iterative process. In each iteration, new versions of the model are created and then the models are evaluated, conclusions are drawn so as to move to the next iteration.

![](figures/MDP_washmachine.png)


## Model Development Process in 10 steps

In this workshop we will go through the process of building a model in ten steps. We will build several candidate models so that they can be compared later using XAI techniques.

![](figures/RML_process.png)


## Step 0. Hello model!

### SARS-COV-2 case study

To demonstrate what responsible predictive modelling looks like, we used data obtained in collaboration with the Polish Institute of Hygiene in modelling mortality after the Covid infection. We realize that data on Coronavirus disease can evoke negative feelings. However, it is a good example of how predictive modelling can directly impact our society and how data analysis allows us to deal with complex, important and topical problems.

Our first model will be based on the statistics collected by the [Centers for Disease Control and Prevention (CDC)](https://www.cdc.gov/). 

![Mortality statistics as presented on the CDC website \url{https://tinyurl.com/CDCmortality} accessed on May 2021. This table shows rate ratios compared to the group 5- to 17-year-olds (selected as the reference group because it has accounted for the largest cumulative number of COVID-19 cases compared to other age groups).](figures/cdc_stats.png){#fig-tableCDC}

**Code snippets:** [https://rml.mi2.ai/00_hello.html](https://rml.mi2.ai/00_hello.html#r-snippets)

## Step 1. Data Exploration (EDA)

### To build a model, we need good data.

In Machine Learning, the word *good* means a large amount of representative data. Unfortunately, collecting representative data is neither easy nor cheap and often requires designing and conducting a specific experiment.

Here we use the data collected through epidemiological interviews. The number of interviewed patients is large, so we treat this data as representative, although unfortunately, this data only involves symptomatic patients who are tested positive for SARS-COV-2. Asymptomatic cases are more likely to be young adults.

The data is divided into two sets: `covid_spring` and `covid_summer`. The first set was acquired in spring 2020 and will be used as training data, while the second dataset was acquired in the summer and will be used for validation. In machine learning, model validation is performed on a separate data set called validation data. This controls the risk of overfitting an elastic model to the training data. If we do not have a separate set, then it is generated using cross-validation, out-of-sample, out-of-time or similar data splitting techniques.

**Code snippets:** [https://rml.mi2.ai/01_eda.html](https://rml.mi2.ai/01_eda.html#r-snippets)

## Step 2. Model Performance

Note that in the Covid-mortality-risk-assessment problem, we are not interested in the binary prediction survived/dead, but rather in the quality of the ranking of risk scores. Relative risks can be used to do a triage, to determine which people need a response most quickly, such as a vaccine.

![Classical measures of performance for classification tasks - from Wikipedia](figures/performance_02.png){#fig-performance_02.png}

For such types of problems, instead of a contingency table, one looks at Receiver Operating Characteristic (ROC) curve, which illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity) at different classification thresholds. The curve's shape and the area under it (AUC-ROC) provide insights into the classifier's discrimination ability and overall predictive accuracy.

![Distribution of scores for the Covid model and the corresponsind ROC curve](figures/02_roc_cdc.png){#fig-roc_cdc}

**Code snippets:** [https://rml.mi2.ai/02_performance.html](https://rml.mi2.ai/02_performance.html#r-snippets)

## Step 3. Grow a tree 

There are hundreds of different methods for training machine learning models available to experienced data scientists. One of the oldest and most popular are tree-based algorithms, first introduced in the book *Classification And Regression Trees* and commonly called CART. 
Here is the general deescription for this class of algorithms.

1. Start with a single node (root) with a full dataset. 
2. For a current node, find a candidate split for the data in this node. To do this, consider every possible variable, and for each variable, consider every possible cutoff (for a continuous variable) or a subset of levels (for a categorical variable). Select the split that maximizes the  measure of separation (see below).
3. Check a stopping criteria like the minimum gain in node purity or depth of a tree. If the stopping criteria are met, then (obviously) stop. Otherwise, partition the current node into two child nodes and go to step 2 for each child node separately.

**Code snippets:** [https://rml.mi2.ai/03_tree.html](https://rml.mi2.ai/03_tree.html#r-snippets)

## Step 4. Plant a forest 

In 2001, Leo Breiman proposed a new family of models, called random forests, which aggregate decisions from an ensemble of trees trained on bootstrap samples. 

Random forests combine two interesting concepts. First, combining multiple weaker models produces a stronger and more stable model. Second, the more diverse the individual models, the greater the benefit of averaging them. To increase the diversity of models, Breiman used the bootstrap technique. Bootstrap is today a very widespread and powerful statistical procedure. 

![The key steps are: to generate a set of B bootstrap copies of the dataset by sampling rows with replacement. Deep trees are trained for each copy. During the prediction, the results of the individual trees are aggregated.](figures/04_randomForest.png){#fig-randomForestFig}

**Code snippets:** [https://rml.mi2.ai/04_forest.html](https://rml.mi2.ai/04_forest.html#r-snippets)

## Step 5. Hyperparameter Optimisation 

Machine Learning algorithms typically have many hyperparameters that specify a model training process. For some model families, like Support Vector Machines (SVM) or Gradient Boosting Machines (GBM), the selection of such hyperparameters has a strong impact on the performance of the final model. The process of finding good hyperparameters is commonly called *tuning*.

Different model families have different sets of hyperparameters. We don't always want to optimize all of them simultaneously, so the first step is to define the hyperparameter search space. Once it is specified, then tuning is based on a looped two steps: (1) select a set of hyperparameters and (2) evaluate how good this set of hyperparameters is. These steps are repeated until some stopping criterion is met, such as the maximum number of iterations, or desired minimum model performance.


![The hyperparameter optimization scheme implemented in the mlr3tuning package. Source: https://mlr3book.mlr-org.com/tuning.html](figures/05_mlr3tuning.png){#fig-mlr3tuning}

**Code snippets:** [https://rml.mi2.ai/05_hpo.html](https://rml.mi2.ai/05_hpo.html#r-snippets)

## Code-snippets

See: [https://rml.mi2.ai/](https://rml.mi2.ai/)


![](figures/rml_eng.png){fig-align="center"}


# Take-home message

<br><br><br>

![](mid/mid_home_01.png)


---

**Why interpretability is important?**

- Higher trust -> **higher adoption of ML/AI solutions** that will support decision making process
- May be **required by auditors, regulators**, law
- New tool for model exploration -> to **gain new insights** about the data/nature of some phenomenom
- Gatekeeping role, human can **control and/or block wrong decisions** when knowing key reasons behind these decisions
- **Debugg/improve data or models**, identify wrong behaviour and help to plan actions to fix it
- **Deeper diagnostic of models**, validation against some domain knowledge, expectations or other values (like human rights -> fairness)


**Goals for this course:**

- Learn XAI techniques (model agnostic)
- Learn the strengths and weaknesses of these techniques while doing a hands-on projects
- Learn how to communicate explanations to (domain) experts and lay users


# References

<br><br><br>

![](mid/mid_references_01.png)

---

## References

- eXplainable Machine Learning course for Machine Learning (MSc) studies at the University of Warsaw. [https://github.com/mim-uw/eXplainableMachineLearning-2023](https://github.com/mim-uw/eXplainableMachineLearning-2023)
- Explanatory Model Analysis. Explore, Explain, and Examine Predictive Models. With examples in R and Python.  [https://ema.drwhy.ai/](https://ema.drwhy.ai/)
- The Hitchhiker's Guide to Responsible Machine Learning. Shorter summary of EMA book. [https://rml.mi2.ai/](https://rml.mi2.ai/)


