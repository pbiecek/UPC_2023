<!DOCTYPE html>
<html lang="en"><head>
<script src="BarcelonaTech_day2_files/libs/clipboard/clipboard.min.js"></script>
<script src="BarcelonaTech_day2_files/libs/quarto-html/tabby.min.js"></script>
<script src="BarcelonaTech_day2_files/libs/quarto-html/popper.min.js"></script>
<script src="BarcelonaTech_day2_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="BarcelonaTech_day2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BarcelonaTech_day2_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="BarcelonaTech_day2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.313">

  <meta name="author" content="Przemyslaw Biecek">
  <title>quarto-input92c63fca</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="BarcelonaTech_day2_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="BarcelonaTech_day2_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="BarcelonaTech_day2_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="BarcelonaTech_day2_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><p><img data-src="figures/barcelona.png" style="width:9in"></p></h1>
  <p class="subtitle">Day 2: SHAP / Break-Down and LIME</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Przemyslaw Biecek 
</div>
</div>
</div>

</section>
<section>
<section id="paper-of-the-day-12" class="title-slide slide level1 center">
<h1>Paper of the day (1/2)</h1>
<p><br><br><br></p>

<img data-src="mid/mid_paper_01.png" class="r-stretch"><div class="cell">
<style type="text/css">
.reveal {
  font-size: 24px;
  line-height: 1.6!important;
}
code {
  font-size: 18px!important;
  line-height: 1.2!important;
}
pre {
  line-height: 1.2!important;
}
</style>
</div>
</section>
<section id="a-unified-approach-to-interpreting-model-predictions" class="slide level2">
<h2>A Unified Approach to Interpreting Model Predictions</h2>
<ul>
<li>In this course, you will learn about the main methods and tools related to XAI, but also (and this may be unique) about selected papers and researchers.</li>
<li>That’s why we will start this and the next classes with a brief presentation of a high-impact article from the XAI field + few words about the author of this article.</li>
<li>Today we are talking about Shapley values, so the article of the day will be the 2017 SHAP method article.</li>
<li>It will be about the paper <a href="https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">A Unified Approach to Interpreting Model Predictions</a></li>
</ul>
<p>
<img src="images/shap_abstract.png" width="100%">
</p>
</section>
<section id="shap-paper-in-numbers" class="slide level2">
<h2>SHAP paper in numbers</h2>
<ul>
<li>This article is really exceptional, it will soon exceed 10,000 citations which is an amazing achievement.</li>
<li>The article has several strong points, which we will talk about later today, one of which is the available software that allows you to easily use the described method</li>
<li>This software is a shap library, which on GitHub has skyrocketing numbers of stars and downloads</li>
</ul>
<p>
<img src="images/shap_popular2.png" width="100%">
</p>
<p>
<img src="images/shap_popular3.png" width="100%">
</p>
</section>
<section id="why-shap" class="slide level2">
<h2>Why SHAP?</h2>
<ul>
<li>Shapley values are currently the most popular technique for model explanations (almost in each category: local, global, model agnostic, model specific…)</li>
<li>if you remember only one method after this course, let it be the SHAP</li>
<li>It has more than five years of development. In the list of major XAI methods, you can also find its various extensions like ShapleyFlow or ASV (more about them later)</li>
<li>figures below are from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2">Explainable AI Methods - A Brief Overview</a></li>
</ul>
<p>
<img src="images/shap_intro1.png" width="100%">
</p>
<p>
<img src="images/shap_intro2.png" width="100%">
</p>
</section>
<section id="xai-pyramid" class="slide level2">
<h2>XAI pyramid</h2>
<ul>
<li>We will use an XAI pyramid to present new methods during this course.</li>
<li>Today we will mainly talk about the method of local explanations - Shapley values, which for a single observation determines the importance of variables.</li>
</ul>
<p>
<img src="images/xai_piramide_shap1.png" width="100%">
</p>
</section>
<section id="xai-pyramid-1" class="slide level2">
<h2>XAI pyramid</h2>
<ul>
<li>This is one of the three fundamental methods of explaining the behaviour of predictive models.</li>
<li>The following three panels introduce these three concepts; we will return to them in one week and two weeks.</li>
<li>SHAP corresponds to panel C. We try to explain the behaviour of the model by decomposing the distance between this particular prediction and the average prediction of the model.</li>
</ul>
<p>
<img src="images/xai_piramide_shap2.png" width="100%">
</p>
</section></section>
<section>
<section id="shapley-values" class="title-slide slide level1 center">
<h1>Shapley values</h1>
<p><br><br><br></p>

<img data-src="mid/mid_players_01.png" class="r-stretch"></section>
<section id="notation" class="slide level2">
<h2>Notation</h2>
<ul>
<li>We have set of <span class="math inline">\(P = \{1, ..., p\}\)</span> players</li>
<li>For each coalition, i.e.&nbsp;subset <span class="math inline">\(S \subseteq P\)</span> we can calculate the payout <span class="math inline">\(v(S)\)</span> and <span class="math inline">\(v(\{\varnothing\}) = 0\)</span></li>
<li>We want to fairly distribute the payout <span class="math inline">\(v(P)\)</span></li>
<li>Optimal attribution for player <span class="math inline">\(i\in P\)</span> will be denoted as <span class="math inline">\(\phi_i\)</span></li>
</ul>
</section>
<section id="motivational-example-13" class="slide level2">
<h2>Motivational example 1/3</h2>
<p>How to divide the reward?</p>
<ul>
<li>Three parties A, B and C took part in the election.</li>
<li>As a result of the election, parties A and B each have 49% representation in the parliament and party C has 2% representation.</li>
<li>Let’s assume that A and C formed a government.</li>
<li>How to fairly divide the prize (ministries)?</li>
<li>What share of the prize should party C have?</li>
</ul>
<p>Note that any two parties can form a government. In that case, should the prize for C be equal to or less than that for A?</p>
<p>
<img src="images/shap_v_01.png" width="100%">
</p>
</section>
<section id="motivational-example-23" class="slide level2">
<h2>Motivational example 2/3</h2>
<p>Students A, B and C carry out a project together. With this payoff table, determine what portion of the award each student should get.</p>
<p>
<img src="images/shap_v_02.png" width="100%">
</p>
</section>
<section id="motivational-example-23-cont." class="slide level2">
<h2>Motivational example 2/3 cont.</h2>
<p>Students A, B and C carry out a project together. With this payoff table, determine what portion of the award each student should get.</p>
<p>
<img src="images/shap_v_03.png" width="100%">
</p>
</section>
<section id="motivational-example-33" class="slide level2">
<h2>Motivational example 3/3</h2>
<p>Students A, B and C carry out a project together. With this payoff table, determine what portion of the award each student should get.</p>
<p>
<img src="images/shap_v_04.png" width="100%">
</p>
</section>
<section id="motivational-example-33-cont." class="slide level2">
<h2>Motivational example 3/3 cont.</h2>
<p>Students A, B and C carry out a project together. With this payoff table, determine what portion of the award each student should get.</p>
<p>
<img src="images/shap_v_05.png" width="100%">
</p>
</section>
<section id="required-properties-of-fair-payout" class="slide level2">
<h2>Required properties of fair payout</h2>
<p>One can define various desirable properties of fair reward distribution. The following seem to be natural (or at least they were for Lord Shapley).</p>
<ul>
<li><strong>Efficiency</strong>: all contributions sum up to the final reward</li>
</ul>
<p><span class="math display">\[
\sum_j \phi_j = v(P)
\]</span></p>
<ul>
<li><strong>Symmetry</strong>: if players <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> contributed in the same way to each coalition then they get the same reward</li>
</ul>
<p><span class="math display">\[
\forall_S v(S \cup \{i\}) = v(S \cup \{j\})     \Rightarrow \phi_i = \phi_j
\]</span></p>
<ul>
<li><strong>Dummy</strong>: if player <span class="math inline">\(i\)</span> does not contribute then its reward is <span class="math inline">\(0\)</span></li>
</ul>
<p><span class="math display">\[
\forall_S v(S \cup \{i\}) = v(S)    \Rightarrow \phi_i = 0
\]</span></p>
<ul>
<li><strong>Additivity</strong>: reward in sum of games <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> is sum of rewards</li>
</ul>
<p><span class="math display">\[
\forall_S v(S) = v_1(S) + v_2(S)    \Rightarrow \phi_i = \phi_{1,i} + \phi_{2,i}
\]</span></p>
</section>
<section id="shapley-values-via-permutations" class="slide level2">
<h2>Shapley values (via permutations)</h2>
<ul>
<li>Fair reward sharing strategy for player <span class="math inline">\(j\in P\)</span> will be denoted as <span class="math inline">\(\phi_j\)</span>. Surprise, these are Shapley values.</li>
<li>Note that added value of player <span class="math inline">\(j\)</span> to coalition <span class="math inline">\(S\)</span> is <span class="math inline">\(v(S \cup \{j\}) - v(S)\)</span></li>
<li>Shapley values are defined as</li>
</ul>
<p><span class="math display">\[
\phi_j = \frac{1}{|P|!} \sum_{\pi \in \Pi} (v(S_j^\pi \cup \{j\}) - v(S_j^\pi))
\]</span></p>
<p>where <span class="math inline">\(\Pi\)</span> is a set of all possible permutations of players <span class="math inline">\(P\)</span> while <span class="math inline">\(S_j^\pi\)</span> is a set of players that are before player <span class="math inline">\(j\)</span> in permutation <span class="math inline">\(\pi\)</span>.</p>
<ul>
<li>Instead of trying all <span class="math inline">\(\Pi\)</span> permutations one can use only <span class="math inline">\(B\)</span> random permutations to estimate <span class="math inline">\(\phi_j\)</span></li>
</ul>
<p><span class="math display">\[
\hat\phi_j = \frac{1}{|B|} \sum_{\pi \in B} (v(S_j^\pi \cup \{j\}) - v(S_j^\pi))
\]</span></p>
</section>
<section id="shapley-values-via-subsets" class="slide level2">
<h2>Shapley values (via subsets)</h2>
<p>
<img src="images/shap_order.png" width="100%">
</p>
<ul>
<li>Once you have a given set <span class="math inline">\(S_j^\pi\)</span> of players that are before <span class="math inline">\(j\)</span> in a permutation <span class="math inline">\(\pi\)</span>, then the added value of <span class="math inline">\(j\)</span> is the same for all permutations that starts with <span class="math inline">\(S_j^\pi\)</span>. There is <span class="math inline">\((|P| - |S_j^\pi| - 1)!\)</span> of such permutations.</li>
<li>Also the order of players in <span class="math inline">\(S_j^\pi\)</span> does not matter as the added value of <span class="math inline">\(j\)</span> is the same for all permutations of <span class="math inline">\(S_j^\pi\)</span>. There is <span class="math inline">\(|S_j^\pi|!\)</span> of such orders.</li>
<li>Formula for Shapley values can be rewritten in a following way</li>
</ul>
<p><span class="math display">\[
\phi_j = \sum_{S \subseteq P / \{j\}}  \frac{|S|! (|P| - |S| - 1)!}{|P|!} (v(S \cup \{j\}) - v(S))
\]</span></p>
<ul>
<li>The advantage is summing over subsets, of which there are <span class="math inline">\(2^p\)</span> instead of permutations, of which there are <span class="math inline">\(p!\)</span>.</li>
</ul>
</section>
<section id="motivational-example-33-solution" class="slide level2">
<h2>Motivational example 3/3 solution</h2>
<p>Students A, B and C carry out a project together. With this payoff table, determine what portion of the award each student should get.</p>
<p>
<img src="images/shap_v_05.png" width="100%">
</p>
<ul>
<li>Now we can calculate the Shapley values and they will be a fair distribution of the reward between students A, B and C</li>
</ul>
<p><span class="math display">\[
\phi_{A} = \frac{1}{6} (10*2 + 20 + 10 + 40*2) = 21 \frac 23
\]</span></p>
<p><span class="math display">\[
\phi_{B} = \frac{1}{6} (30*2 + 40 + 10 + 40*2) = 31 \frac 23
\]</span></p>
<p><span class="math display">\[
\phi_{C} = \frac{1}{6} (50*2 + 50 + 30 + 50*2) = 46 \frac 23
\]</span></p>
</section></section>
<section>
<section id="shapley-values-for-machine-learning-models" class="title-slide slide level1 center">
<h1>Shapley values for Machine Learning Models</h1>
<p><br><br><br></p>

<img data-src="mid/mid_players_01.png" class="r-stretch"></section>
<section id="definitions" class="slide level2">
<h2>Definitions</h2>
<ul>
<li><p>Let’s start with local explanations, focused on single point <span class="math inline">\(x\)</span> and the model prediction <span class="math inline">\(f(x)\)</span>.</p></li>
<li><p>Now instead of players, you can think about variables. We will distribute a reward between variables to recognize their contribution to the model prediction <span class="math inline">\(f(x)\)</span>.</p></li>
<li><p>Reward to be distributed among players:</p></li>
</ul>
<p><span class="math display">\[
f(x) - E f(x)
\]</span></p>
<div class="fragment">
<ul>
<li>Payoff value function for coalition <span class="math inline">\(S\)</span></li>
</ul>
<p><span class="math display">\[
v(S) = f_S(x_S) - E f(x)
\]</span> where <span class="math inline">\(f_S(x_S)\)</span> is the model prediction maginalized over <span class="math inline">\(P/S\)</span> variables, i.e. <span class="math display">\[
f_S(x_S) = \int_{X_{-S}} f(x_S, X_{-S}) dP(X_{-S})
\]</span></p>
</div>
<div class="fragment">
<ul>
<li>Shapley values via permutations</li>
</ul>
<p><span class="math display">\[
\phi_j = \frac{1}{|P|!} \sum_{\pi \in \Pi} v(S_j^\pi \cup \{j\}) - v(S_j^\pi)
\]</span></p>
<p><strong>Note:</strong> <span class="math inline">\(|P|!\)</span> grows quite fast. <span class="math inline">\(10! = 3 628 800\)</span>. Good news: instead of checking all permutations, one can focus on random <span class="math inline">\(M\)</span> permutations. Also calculation of <span class="math inline">\(f_S(x_S)\)</span> may be computationally heavy for large datasets. But it may be approximated on a subset of observations.</p>
</div>
</section>
<section id="how-to-understand-the-value-function" class="slide level2">
<h2>How to understand the value function</h2>
<ul>
<li>Let’s take a look at how the value function works for a set S of players using the Titanic data example and an explanation for the observations age=8, class=1st, fare=72, ….</li>
<li>Let’s consider the process of conditioning the distribution of data on consecutive variables. In the figure below, we start the prediction distribution for all data, this corresponds to a coalition without players.</li>
<li>Then we add the player <code>age</code>, which means conditioning the data with the condition <code>age=8</code>. Implementation-wise, assuming the independence of the variables, this would correspond to replacing the age value in each observation with the value 8.</li>
<li>Next, we add the class variable to the coalition, which means further conditioning the data with the condition <code>class=1st</code>. In the next step, we add fare to the coalition, and so on.</li>
<li>In the last step, once all the players are in the coalition, that is, all the variables, the model’s predictions will reduce to a single point <span class="math inline">\(f(x)\)</span></li>
</ul>
<p>
<img src="images/xai_bd_1.png" width="100%">
</p>
<ul>
<li>In fact, we are not interested in the distributions of conditional predictions, only in the expected value of these distributions. This is what our value function is.</li>
</ul>
<p>
<img src="images/xai_bd_2.png" width="100%">
</p>
<ul>
<li>The added value of variable <span class="math inline">\(j\)</span> when added to the coalition <span class="math inline">\(S\)</span> is the change in expected value. In the example below, adding the <code>class</code> variable to a coalition with the <code>age</code> variable increases the reward by <span class="math inline">\(0.086\)</span>.</li>
</ul>
<p>
<img src="images/xai_bd_3.png" width="100%">
</p>
</section>
<section id="average-of-conditional-contributions" class="slide level2">
<h2>Average of conditional contributions</h2>
<ul>
<li>The Shapley value is the average after all (or a large number) of the orders in which variables are added to the coalition.</li>
<li>For diagnostic purposes, on graphs, we can also highlight the distribution of added values for different coalitions to get information on how much the effect of a given variable is additive, i.e.&nbsp;leads to the same added value regardless of the previous composition of the coalition.</li>
</ul>
<p>
<img src="images/xai_bd_4.png" width="100%">
</p>
<ul>
<li>Order matters. For a model that allows interactions, it is easy to find an example of a non-additive effect of a variable. How to explain the different effects of the age variable in the figure below?</li>
</ul>
<p>
<img src="images/xai_bd_5.png" width="100%">
</p>
</section>
<section id="shap-values" class="slide level2">
<h2>SHAP values</h2>
<p><span class="math display">\[
\phi_j = \frac{1}{|P|!} \sum_{\pi \in \Pi} v(S_j^\pi \cup \{j\}) - v(S_j^\pi)
\]</span></p>
<ul>
<li>The <span class="math inline">\(v(S \cup \{j\}) - v(S)\)</span> may be approximated with <span class="math inline">\(\hat f_{S \cup \{j\}}(x^*) - \hat f_S(x^*)\)</span> where</li>
</ul>
<p><span class="math display">\[
\hat f_S(x^*) = \sum_{i=1}^N f(x^*_S, x^i_{-S})
\]</span></p>
<ul>
<li>The exact calculation of Shapley values leads to the formula</li>
</ul>
<p><span class="math display">\[
\phi_j(x^*) = \frac{1}{N |P|!} \sum_{\pi \in \Pi} \sum_{i=1}^{N}  f(x^*_{S^\pi \cup \{j\}}, x^i_{-S^\pi \cup \{j\}}) - f(x^*_{S^\pi}, x^i_{-S^\pi})
\]</span></p>
<ul>
<li><strong>Note:</strong> For estimation, one can use an only subset of permutations from <span class="math inline">\(\Pi\)</span> and a subset of observations <span class="math inline">\(\{1, ..., N\}\)</span>.</li>
</ul>
</section>
<section id="kernel-shap" class="slide level2">
<h2>Kernel SHAP</h2>
<ul>
<li>Accurate calculation of Shapley values is a very time-consuming task.</li>
<li>The Kernel-SHAP method makes it possible to estimate these values at a lower computational cost - and thus faster.</li>
<li>You can think of it as an adaptation of the LIME method. The explanation, too, is a linear model approximation of the model in an interpretable feature space.</li>
<li>The interpretable variable space is a binary space describing whether a variable enters a coalition or not. If it enters the coalition then we use the value of this variable from the observation being explained. If it doesn’t then we sample a value from the dataset in its place.</li>
<li>We compute Shapley values by weighted linear regression using an interpretable representation of the variables as input. Linear regression coefficients are estimates of Shapley values.</li>
</ul>
<p>
<img src="images/shap_kernel.png" width="100%">
</p>
</section>
<section id="tree-shap" class="slide level2">
<h2>Tree SHAP</h2>
<p>Trees have nice structure, it makes them easier to analyse with Shapley values.</p>
<p>For a model that is a weighted sum of trees (bagging, boosting, random forest) the Shapley values for the model are weighted Shapley values for each tree.</p>
<p>Let’s consider a brute force algorithm for a single tree (processing from leaves to the root):</p>
<ul>
<li>for a leaf, it returns the value in the leaf,</li>
<li>for a node with a variable from S it returns the value of a left or right node given the variable’s value,</li>
<li>for a node without a variable from S it returns the weighted average of the left and right nodes.</li>
</ul>
<p>The brute force algorithm has complexity <span class="math inline">\(O(2^m)\)</span></p>
<p>but one can go down to <span class="math inline">\(O(XTLD^2) = O(TLD \cdot XD)\)</span></p>
</section>
<section id="tree-shap---an-example" class="slide level2">
<h2>Tree SHAP - an example</h2>
<ul>
<li>Let’s calculate <span class="math inline">\(val(S)\)</span> for <code>x = (age: 5, fare:20, sibsp:2)</code>.</li>
</ul>
<p>
<img src="images/xai_bd_6.png" width="100%">
</p>
<ul>
<li>where</li>
</ul>
<p><span class="math display">\[
v(S) = \int_{X_{-S}} f(x_S, X_{-S}) dP(X_{-S}) - E f(x)
\]</span></p>
</section>
<section id="from-local-to-global-feature-importance" class="slide level2">
<h2>From local to global – Feature importance</h2>
<ul>
<li>The SHAP method gives local explanations, i.e.&nbsp;explanations for each single observation. But we can convert them to global explanations by aggregating the explanations for individual observations.</li>
<li>For example, we can assess the validity of a variable by counting the average modulus of SHAP explanations.</li>
<li>Such a measure of the importance of variables does not depend on the model structure and can be used to compare models.</li>
<li>Below is an example for the model trained for Titanic data</li>
</ul>
<p>
<img src="images/shap_global_3.png" width="100%">
</p>
</section>
<section id="from-local-to-global-summary-plot" class="slide level2">
<h2>From local to global – Summary plot</h2>
<ul>
<li>One of the most useful statistics is a plot summarizing the distribution of Shapley values for the data for each variable.</li>
<li>On the OX axis are presented the Shapley values, in the rows are the variables. The color indicates whether an observation had a high or low value in that variable.</li>
<li>From the graph you can read which variables are important (they have a large spread of points)</li>
<li>You can read what is the relationship between the variable and the Shapley value, whether the color has a monotonic gradation or there are some dependencies</li>
<li>You can read the distribution of Shapley values</li>
</ul>
<p>
<img src="images/shap_global_2.png" width="100%">
</p>
</section>
<section id="from-local-to-global-dependence-plot" class="slide level2">
<h2>From local to global – Dependence plot</h2>
<ul>
<li>If we plot the Shapley values as functions of the value of the original variable, it is possible to see what kind of relationship exists between this variable and the average result.</li>
<li>This type of plots allows you to choose the transformations of the variable, and better understand the relationship between this variable and the result of the model</li>
</ul>
<p>
<img src="images/shap_global_4.png" width="100%">
</p>
<ul>
<li>We can additionally color the graph depending on one more variable (in the example below, it is gender) to see if an interaction is present in the model. In this case, the attributes of the model will depend on the value of this additional variable.</li>
</ul>
<p>
<img src="images/shap_global_5.png" width="100%">
</p>
</section></section>
<section>
<section id="paper-of-the-day-22" class="title-slide slide level1 center">
<h1>Paper of the day (2/2)</h1>
<p><br><br><br></p>

<img data-src="mid/mid_paper_01.png" class="r-stretch"></section>
<section id="why-should-i-trust-you-explaining-the-predictions-of-any-classifier" class="slide level2">
<h2>“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</h2>
<ul>
<li>In this course, you will learn about the XAI methods and tools, but also about selected papers and researchers.</li>
<li>Today we will talk about LIME method, so the article of the day will be the LIME paper from 2016: <a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">“Why Should I Trust You?” Explaining the Predictions of Any Classifier</a></li>
</ul>
<p>
</p><center>
<img src="images/lime_abstract.png" width="80%">
</center>
<p></p>
</section>
<section id="lime-paper-in-numbers" class="slide level2">
<h2>LIME paper in numbers</h2>
<ul>
<li>The LIME paper has over 10k citations</li>
<li>The <code>lime</code> python package has today over 10k start on github</li>
<li>The <em>husky example</em> is now the most frequently presented example calling for debugging models (more on this later)</li>
</ul>
<p>
<img src="images/lime_popular2.png" width="100%">
</p>
<p>
<img src="images/lime_popular3.png" width="100%">
</p>
</section>
<section id="why-lime" class="slide level2">
<h2>Why LIME?</h2>
<ul>
<li>Gives sparse explanations based on an interpretable data space</li>
<li>Very popular, especially for computer vision / NLP tasks</li>
<li>Very tempting approach – explain a complex model by a simpler surrogate (although intuition can be deceptive here)</li>
</ul>
<p>
<img src="images/shap_intro1.png" width="100%">
</p>
<p>
<img src="images/shap_intro2.png" width="100%">
</p>
<ul>
<li>Figures below are from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2">Explainable AI Methods - A Brief Overview</a></li>
</ul>
</section>
<section id="xai-pyramid-2" class="slide level2">
<h2>XAI pyramid</h2>
<ul>
<li>LIME is based on one of the three fundamental approaches to explanation of predictive models.</li>
<li>LIME corresponds to panel B – approximation with linear surrogate model to get some understanding about black-box model behavior around <span class="math inline">\(x\)</span></li>
</ul>
<p>
<img src="images/xai_piramide_shap2.png" width="100%">
</p>
</section></section>
<section>
<section id="lime---local-interpretable-model-agnostic-explanations" class="title-slide slide level1 center">
<h1>LIME - Local Interpretable Model-agnostic Explanations</h1>
<p><br><br><br></p>

<img data-src="mid/mid_limes_01.png" class="r-stretch"></section>
<section id="start-with-why" class="slide level2">
<h2>Start with Why</h2>
<p>Desired characteristics of explanations (from LIME paper)</p>
<ul>
<li>Explanations should be easy to undestand = interpretable (simple, sparse, based on interpretable features) for a user</li>
<li>Good explanation should be model-agnostic, i.e.&nbsp;does not depend on model structure. This will help to compare explanations for different models</li>
<li>Local fidelity of explanations</li>
</ul>
<p>
</p><center>
<img src="images/lime_intro.png" width="85%"><br> Explanation process. Figure from LIME paper
</center>
<p></p>
</section>
<section id="core-idea" class="slide level2">
<h2>Core idea</h2>
<p>The core ideas behind LIME are:</p>
<ul>
<li>Input to the model will be transformed into an interpretable feature space</li>
<li>Local model behaviour will be explained by approximating it by an interpretable surrogate model (e.g.&nbsp;a shallow tree or a linear regression model)</li>
<li>Local approximation is trained on artificial points generated from the neighborhood of the observation of interest <span class="math inline">\(x\)</span></li>
</ul>
<p>
</p><center>
<img src="images/lime_introduction.png" width="50%">
</center>
<br>Figure from EMA book
<p></p>
</section>
<section id="fidelity-interpretability-trade-off" class="slide level2">
<h2>Fidelity-Interpretability Trade-off</h2>
<p>The explanation will be a model <span class="math inline">\(g\)</span> that approximates the behavior of the complex model <span class="math inline">\(f\)</span> and is as simple as possible</p>
<p><span class="math display">\[
\hat g = \arg \min_{g \in G} L\{f, g, \pi(x)\} + \Omega(g)
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(f()\)</span> is a model to be explained</li>
<li><span class="math inline">\(x\)</span> is an observation of interest</li>
<li><span class="math inline">\(G\)</span> is a class of interpretable models</li>
<li><span class="math inline">\(\hat g\)</span> is an explanation, a model from class <span class="math inline">\(G\)</span></li>
<li><span class="math inline">\(\Omega(g)\)</span> is a penalty function that measures complexity of models from <span class="math inline">\(G\)</span>. For regression models it could be the number of non-zero coefficients, for trees the number of nodes. For simplicity, we will consider a family of models <span class="math inline">\(G\)</span> such that all models in this family have complexity <span class="math inline">\(K\)</span></li>
<li><span class="math inline">\(L()\)</span> a function measuring the discrepancy between models <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> in the neighborhood <span class="math inline">\(\pi(x^*)\)</span></li>
</ul>
</section>
<section id="lime-algorithm" class="slide level2">
<h2>LIME Algorithm</h2>
<p>Explanations can be calculated with a following instructions.</p>
<ol type="1">
<li>Let <span class="math inline">\(x'\)</span> = <span class="math inline">\(h\)</span>(x) be a version of <span class="math inline">\(x\)</span> in the interpretable data space</li>
<li>for i in 1…N {</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;z’[i] = <code>sample_around</code>(x’)</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y’[i] = <span class="math inline">\(f\)</span>(z’[i])</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w’[i] = <code>similarity</code>(x’, z’[i])</li>
<li>}</li>
<li>return <code>K-LASSO</code>(y’, x’, w’)</li>
</ol>
<p>where</p>
<ul>
<li><span class="math inline">\(x\)</span> – an observation to be explained</li>
<li><span class="math inline">\(N\)</span> – sample size needed to fit a glass-box model</li>
<li><span class="math inline">\(K\)</span> – complexity, the maximum number of variables in the glass-box model</li>
<li><code>similarity</code> – a distance function in the original data space</li>
<li><code>K-LASSO</code> – a weighted LASSO linear-regression model that selects K variables</li>
<li>w’ – weights that measure of the similarity between original observation <span class="math inline">\(x\)</span> and new artificially generated observations. Weights may be based on <span class="math inline">\(\exp(-d)\)</span> function, where <span class="math inline">\(d\)</span> is an Euclidean distance, cosine distance or other distance measure (depending on the data structure),</li>
</ul>
</section>
<section id="example-duck-or-horse-14" class="slide level2">
<h2>Example: Duck or horse? 1/4</h2>
<p>
</p><center>
<img src="images/lime_intro2.png" width="30%">
</center>
<p></p>
<p>Let’s see how LIME can be used to solve this problem.</p>
<p><strong>Initial settings</strong></p>
<ul>
<li>Let’s consider a VGG16 neural network trained on the ImageNet data</li>
<li>Input size are images 244 <span class="math inline">\(\times\)</span> 244 pixels. We have 1000 potential categories for the training data</li>
<li>The input space is of dimension 3 <span class="math inline">\(\times\)</span> 244 <span class="math inline">\(\times\)</span> 244, i.e.&nbsp;it is a 178 608-dimensional space</li>
<li>We need to translate the input to the interpretable data space, here image will be transformed into superpixels, which are treated as binary features (see an example later)</li>
<li>In this example <span class="math inline">\(f()\)</span> operates on space with <span class="math inline">\(178 608\)</span> dimensions, while the glass-box model <span class="math inline">\(g()\)</span> operates on a binary space with <span class="math inline">\(100\)</span> dimensions</li>
<li>We will ask for explanations of complexity 10</li>
</ul>
</section>
<section id="example-duck-or-horse-24" class="slide level2">
<h2>Example: Duck or horse? 2/4</h2>
<p><strong>Interpretable data space</strong></p>
<ul>
<li>Interpretable data space is a binary space that encodes presence or absence of selected features</li>
<li>The interpretable space can be constructed globally (e.g.&nbsp;for tabular data) or locally (e.g.&nbsp;for images)</li>
<li>For image data, the most common approach constructs an interpretable data space for each observation separately by using a segmentation algorithm.</li>
<li>The result is the division of the input image into a certain number of regions/called superpixels</li>
</ul>
<p>
<img src="images/lime_ex_1.png" width="100%">
</p>
</section>
<section id="example-duck-or-horse-34" class="slide level2">
<h2>Example: Duck or horse? 3/4</h2>
<p><strong>Sampling around x</strong></p>
<ul>
<li>We sample around the observation x’ in the interpretable space</li>
<li>Since it’s a binary space in which an observation <span class="math inline">\(x\)</span> is represented by a vector of ones</li>
<li>Sampling corresponds to selecting randomly coordinates that will be flipped to zero</li>
<li>We need N of such new observations</li>
</ul>
<p>
<img src="images/lime_ex_2.png" width="100%">
</p>
</section>
<section id="example-duck-or-horse-44" class="slide level2">
<h2>Example: Duck or horse? 4/4</h2>
<p><strong>Fitting of an interpretable model</strong></p>
<ul>
<li>For new data, we make predictions with model <span class="math inline">\(f()\)</span></li>
<li>And then for the observations in the interpretable representation we train a K-LASSO model which will have <span class="math inline">\(K\)</span> non-zero coefficients</li>
<li>We can use the <span class="math inline">\(R^2\)</span> coefficient to assess the quality of fit of the model <span class="math inline">\(g()\)</span></li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_3.png" width="75%">
</center>
<p></p>
</section>
<section id="interpretable-data-representations" class="slide level2">
<h2>Interpretable data representations</h2>
<p>How to transform the input data into a binary vector of shorter length?</p>
<ul>
<li>For image data interpretable feature space is commonly based on superpixels, i.e.&nbsp;through image segmentation</li>
<li>For text data, words or groups of words are frequently used as interpretable variables</li>
<li>For tabular data, continuous variables are often discretized to obtain interpretable bianary variables. In the case of categorical variables, combination of levels is used to get bianary variables.</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_8.png" width="75%">
</center>
<br> Example from <a href="https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html">LIME github</a>
<p></p>
</section>
<section id="model-debugging-13" class="slide level2">
<h2>Model debugging 1/3</h2>
<ul>
<li>There are many reasons to know and develop XAI techniques</li>
<li>One of them is the ability to debug the model</li>
<li>The most well-known example is improving the performance of a network that misclassified the following image</li>
<li>How LIME can help here?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_9.png" width="25%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
</section>
<section id="model-debugging-23" class="slide level2">
<h2>Model debugging 2/3</h2>
<ul>
<li>The model works very well. Classification between husky of wolf in accurate in almost every image except one. Why?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_6.png" width="70%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
</section>
<section id="model-debugging-33" class="slide level2">
<h2>Model debugging 3/3</h2>
<ul>
<li>Can LIME’s explanation help us find the source of the problem?</li>
<li>It turns out that in the case of classification as a wolf, the important feature is the snow in the background</li>
<li>Effectively, the model has learned to recognize snow in the background and so classifies as a wolf class</li>
<li>This is not a feature that people use for classification wolf/husky. But would you sacrifice the quality of the model to remove the dependence on using the background for classification?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_5.png" width="100%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
<ul>
<li>This story has a happy ending. Proper training that cancelled the model dependency on the snow feature improved the accuracy of the model</li>
</ul>
</section></section>
<section>
<section id="from-local-to-global" class="title-slide slide level1 center">
<h1>From Local to Global</h1>
<p><br><br><br></p>

<img data-src="mid/mid_limes_01.png" class="r-stretch"></section>
<section id="explaining-through-examples" class="slide level2">
<h2>Explaining through examples</h2>
<p>The LIME method was designed to explain the model’s behavior locally, around the observation of interest. But we are often interested in knowing or at least getting an intuition about how the model works globally.</p>
<p>The LIME paper proposes two approaches to globalizing LIME. Both are based on selecting some subset of observations that will be fairly representative of the entire dataset. Assuming the user has time to look at LIME explanations for B observations, the question is how to select them.</p>
<p><strong>Submodular pick (SP) algorithm</strong></p>
<p>
</p><center>
<img src="images/lime_pick.png" width="75%">
</center>
<p></p>
<p>Criterion for selecting observations for global explanations</p>
<p><span class="math display">\[
c(V, W, I) = \sum_{j \in P'} 1_{\exists i\in V; W_{i,j} \neq 0} I_j
\]</span></p>
<p>where <span class="math inline">\(I_j\)</span> is feature importance for feature <span class="math inline">\(i\)</span> while <span class="math inline">\(P'\)</span> is a set of features in an interpretable data space.</p>
<p>The LIME paper presents a user-study example where the submodular picks method most effectively convinces the user how the model works.</p>
</section>
<section id="can-non-experts-improve-a-classifier" class="slide level2">
<h2>Can non-experts improve a classifier?</h2>
<ul>
<li>The LIME paper describes the results of several experiments involving humans subjects</li>
<li>Very interesting results involved using explanations to improve the model, even if the improvement is generated by the knowledge and actions of non-ML-experts</li>
<li>The experiment was based on a model for a classification task based on text data</li>
<li>The explanations of the model generated by the LIME method were then shown to the participants of the experiment. That is, for each observation, the relevant words were highlighted</li>
<li>Participants could determine that some of these words were artifacts and should not be used by the model</li>
<li>The model was then trained again on the remaining features, with the artifacts removed</li>
<li>It turns out that such feature engineering using experts led to better results after several rounds</li>
</ul>
<p>
</p><center>
<img src="images/lime_fe.png" width="60%">
</center>
<br>Figure from the LIME paper
<p></p>
</section></section>
<section>
<section id="anchors" class="title-slide slide level1 center">
<h1>Anchors</h1>
<p><br><br><br></p>

<img data-src="mid/mid_limes_01.png" class="r-stretch"></section>
<section id="anchors-high-precision-model-agnostic-explanations-13" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 1/3</h2>
<ul>
<li>A limitation of the LIME method is the assumption that locally the behavior of a complex model can be explained by approximating it with an additive linear regression model.</li>
<li>But if there are significant interactions in the model then a local approximation with an additive model will not reflect the model’s behavior well.</li>
<li>In the example below, we can see that the word ‘not’ has no additive effect, so its effect does not additively compound with the word ‘bad’.</li>
<li>To explain the model’s behavior, we need a rule that contains both variables.</li>
</ul>
<p>
</p><center>
<img src="images/lime_anchors.png" width="60%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section>
<section id="anchors-high-precision-model-agnostic-explanations-23" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 2/3</h2>
<ul>
<li><p>In <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors: High-Precision Model-Agnostic Explanations</a>, the authors present an alternative explanation method that finds the shortest subset of conditions sufficient to explain the local behavior of the model.</p></li>
<li><p>The intuition behind the anchor is as follows: for a given observation anchors are <em>“sufficient”</em> conditions for a model prediction.</p></li>
<li><p>More formally: <span class="math inline">\(A\)</span> is a rule = set of logical conditions. We will say that <span class="math inline">\(A\)</span> is an <em>anchor</em> if <span class="math inline">\(A(x) = 1\)</span>, i.e.&nbsp;observation <span class="math inline">\(x\)</span> fulfill all these conditions and probability that rule <span class="math inline">\(A\)</span> is true around <span class="math inline">\(x\)</span> is higher than <span class="math inline">\(\tau\)</span>, i.e.</p></li>
</ul>
<p><span class="math display">\[
E_{D(z|A)}[1_{f(x) = f(z)}] \geq \tau,
\]</span></p>
<p>where <span class="math inline">\(D(z|A)\)</span> is a distribution of points that fulfill the rule <span class="math inline">\(A\)</span>.</p>
<p>
</p><center>
<img src="images/lime_anchors2.png" width="60%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section>
<section id="anchors-high-precision-model-agnostic-explanations-33" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 3/3</h2>
<ul>
<li>,,sufficient’’ explanation</li>
<li>If these segments are in the picture then it is classified as a beagle</li>
</ul>
<p>
</p><center>
<img src="images/anchors_1.png" width="90%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section></section>
<section>
<section id="take-home-message" class="title-slide slide level1 center">
<h1>Take-home message</h1>
<p><br><br><br></p>

<img data-src="mid/mid_home_01.png" class="r-stretch"></section>
<section class="slide level2">

<ul>
<li><p>Shapley’s values are based on a concept with roots in <strong>cooperative game theory</strong></p></li>
<li><p>We treat <strong>variables as players</strong> who, in coalitions, influence the prediction of the model</p></li>
<li><p>Shapley’s values result in <strong>additive decomposition of the reward</strong> which is <span class="math inline">\(f(x) - E f(x)\)</span></p></li>
<li><p>Shapley’s values can be calculated in a model-agnostic fashion, but for some models (linear models, tree-based models) there are more efficient ways to estimate such values</p></li>
<li><p><strong>From local to global</strong>. Based on local explanations, global explanations can be constructed</p></li>
<li><p>In SHAP quite often <strong>each variable used in the model has non zero attributions</strong>. For models with many variables, this can be a problem</p></li>
<li><p>LIME method explains local model behavior with <strong>interpretable linear surrogate model</strong></p></li>
<li><p>LIME generates <strong>sparse explanations</strong>, with K features</p></li>
<li><p>Explanations use an <strong>interpretable feature space</strong>, like superpixels for image data, tokens for NLP, quartiles for numeric tabular data</p></li>
</ul>
</section></section>
<section>
<section id="hands-on" class="title-slide slide level1 center">
<h1>Hands-on</h1>
<p><br><br><br></p>

<img data-src="mid/mid_hands_01.png" class="r-stretch"></section>
<section id="step-8.-shapley-values-and-the-break-down-plots" class="slide level2">
<h2>Step 8. Shapley values and the Break-down plots</h2>
<h3 id="sars-cov-2-case-study">SARS-COV-2 case study</h3>
<p><strong>Code snippets:</strong> <a href="https://rml.mi2.ai/08_shap.html">https://rml.mi2.ai/08_shap.html</a></p>

<img data-src="figures/comic_10.png" class="r-stretch quarto-figure-center"></section></section>
<section>
<section id="references" class="title-slide slide level1 center">
<h1>References</h1>
<p><br><br><br></p>

<img data-src="mid/mid_references_01.png" class="r-stretch"></section>
<section id="references-1" class="slide level2">
<h2>References</h2>
<ul>
<li>eXplainable Machine Learning course for Machine Learning (MSc) studies at the University of Warsaw. <a href="https://github.com/mim-uw/eXplainableMachineLearning-2023">https://github.com/mim-uw/eXplainableMachineLearning-2023</a></li>
<li>Explanatory Model Analysis. Explore, Explain, and Examine Predictive Models. With examples in R and Python. <a href="https://ema.drwhy.ai/">https://ema.drwhy.ai/</a></li>
<li>The Hitchhiker’s Guide to Responsible Machine Learning. Shorter summary of EMA book. <a href="https://rml.mi2.ai/">https://rml.mi2.ai/</a></li>
</ul>

<img src="figures/XAI.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p>BarcelonaTech Summer School – 20/06/2023</p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="BarcelonaTech_day2_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>